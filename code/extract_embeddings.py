# -*- coding: utf-8 -*-
"""ACL2023 - Extract_Embeddings.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fIPw6yZdGWKAm7D9MDZ2zCA0ziL4SgkE
"""



from transformers import AutoTokenizer, AutoModel
import torch
import numpy as np

titles = []
contents = []

# 读取数据
with open("ceb_all_data.txt", "r", errors='ignore') as file:
    file_contents = file.readlines()
    for i in file_contents:
        parsed_text = i.split(',', 2)
        parsed_text[0] = parsed_text[0].strip()
        print(f"处理文档: {parsed_text[0]}")
        titles.append(parsed_text[0].strip())
        contents.append(parsed_text[2].strip())


def mean_pooling(model_output, attention_mask):
    """
    实现与sentence_transformers相同的平均池化功能
    将每个token的嵌入向量平均，得到句子嵌入
    """
    token_embeddings = model_output[0]  # 第一个元素包含所有token的嵌入
    # 扩展注意力掩码以匹配嵌入维度
    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()
    # 对token嵌入进行加权求和
    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)
    # 确保分母不为零
    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)
    # 返回平均嵌入
    return sum_embeddings / sum_mask


def encode_sentences(sentences, model_name="bert-base-multilingual-uncased", max_length=512):
    """
    编码句子列表，返回嵌入向量
    参数:
        sentences: 要编码的句子列表
        model_name: 预训练模型名称
        max_length: 最大序列长度
    返回:
        embeddings: 句子嵌入矩阵
    """
    print(f"加载模型: {model_name}")
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModel.from_pretrained(model_name)

    # 设置模型为评估模式
    model.eval()

    all_embeddings = []
    batch_size = 16  # 根据你的GPU内存调整

    print(f"开始编码 {len(sentences)} 个句子...")

    for i in range(0, len(sentences), batch_size):
        batch_sentences = sentences[i:i + batch_size]

        # 对当前批次进行分词
        encoded_input = tokenizer(
            batch_sentences,
            padding=True,
            truncation=True,
            max_length=max_length,
            return_tensors='pt'
        )

        # 移动到GPU（如果可用）
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        model.to(device)
        encoded_input = {k: v.to(device) for k, v in encoded_input.items()}

        # 计算嵌入（不计算梯度）
        with torch.no_grad():
            model_output = model(**encoded_input)

        # 应用平均池化得到句子嵌入
        batch_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])

        # 移回CPU并添加到列表
        all_embeddings.append(batch_embeddings.cpu())

        if (i // batch_size) % 10 == 0:
            print(f"  已处理: {min(i + batch_size, len(sentences))}/{len(sentences)}")

    # 合并所有批次的嵌入
    embeddings = torch.cat(all_embeddings, dim=0)
    print(f"编码完成，嵌入形状: {embeddings.shape}")

    return embeddings


def save_embeddings(embeddings, titles, output_file='ceb_mbert_features.csv'):
    """
    保存嵌入向量到CSV文件，并可选保存标题信息
    """
    # 转换为numpy数组
    embeddings_np = embeddings.numpy()

    print(f"保存嵌入到: {output_file}")
    print(f"嵌入维度: {embeddings_np.shape}")

    # 保存嵌入向量
    np.savetxt(output_file, embeddings_np, delimiter=',')

    # 可选：保存标题信息到单独文件
    title_file = output_file.replace('.csv', '_titles.txt')
    with open(title_file, 'w', encoding='utf-8') as f:
        for title in titles:
            f.write(f"{title}\n")

    print(f"标题信息保存到: {title_file}")
    print("保存完成!")

    # 显示一些统计信息
    print(f"\n统计信息:")
    print(f"  嵌入数量: {embeddings_np.shape[0]}")
    print(f"  每个嵌入维度: {embeddings_np.shape[1]}")
    print(f"  第一个嵌入样例 (前10维): {embeddings_np[0, :10]}")


# 主执行流程
def main():
    print("=" * 60)
    print("开始提取句子嵌入")
    print("=" * 60)

    # 检查是否有可用的GPU
    if torch.cuda.is_available():
        print(f"使用GPU: {torch.cuda.get_device_name(0)}")
    else:
        print("使用CPU")

    # 1. 生成嵌入
    embeddings = encode_sentences(contents, model_name="./bert-model")

    # 2. 保存结果
    save_embeddings(embeddings, titles, 'ceb_mbert_features.csv')

    print("\n" + "=" * 60)
    print("嵌入提取完成!")
    print("=" * 60)


# 如果直接运行此脚本，则执行main函数
if __name__ == "__main__":
    main()


# 可选：提供一个简单的使用示例
def example_usage():
    """
    如何使用生成的嵌入的示例
    """
    # 加载保存的嵌入
    loaded_embeddings = np.loadtxt('ceb_mbert_features.csv', delimiter=',')

    print(f"加载的嵌入形状: {loaded_embeddings.shape}")

    # 示例：计算两个文档之间的余弦相似度
    if loaded_embeddings.shape[0] >= 2:
        from numpy.linalg import norm

        emb1 = loaded_embeddings[0]
        emb2 = loaded_embeddings[1]

        # 计算余弦相似度
        cosine_sim = np.dot(emb1, emb2) / (norm(emb1) * norm(emb2))
        print(f"文档1和文档2的余弦相似度: {cosine_sim:.4f}")